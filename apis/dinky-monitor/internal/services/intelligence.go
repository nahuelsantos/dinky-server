package services

import (
	"context"
	"fmt"
	"math"
	"math/rand"
	"sort"
	"time"

	"github.com/google/uuid"
	"go.uber.org/zap"

	"dinky-monitor/internal/models"
)

// IntelligenceService handles AI/ML-powered analytics
type IntelligenceService struct {
	logger              *zap.Logger
	anomalyModels       map[string]*models.AnomalyDetectionModel
	activeAnalyses      map[string]*models.RootCauseAnalysis
	capacityPlans       map[string]*models.CapacityPlan
	performanceBaseline map[string]models.InsightMetrics
	predictiveAlerts    []*models.PredictiveAlert
	recommendations     []*models.Recommendation
	intelligenceMetrics models.IntelligenceMetrics
}

// NewIntelligenceService creates a new intelligence service
func NewIntelligenceService(logger *zap.Logger) *IntelligenceService {
	service := &IntelligenceService{
		logger:              logger,
		anomalyModels:       make(map[string]*models.AnomalyDetectionModel),
		activeAnalyses:      make(map[string]*models.RootCauseAnalysis),
		capacityPlans:       make(map[string]*models.CapacityPlan),
		performanceBaseline: make(map[string]models.InsightMetrics),
		predictiveAlerts:    make([]*models.PredictiveAlert, 0),
		recommendations:     make([]*models.Recommendation, 0),
		intelligenceMetrics: models.IntelligenceMetrics{},
	}

	// Initialize with default ML models
	service.initializeModels()
	service.initializeBaselines()

	return service
}

// initializeModels sets up default ML models
func (s *IntelligenceService) initializeModels() {
	now := time.Now()

	models := []*models.AnomalyDetectionModel{
		{
			ID:       uuid.New().String(),
			Name:     "Statistical Anomaly Detector",
			Type:     "statistical",
			Status:   "active",
			Accuracy: 0.92,
			TrainingData: models.TrainingDataset{
				Source:      "prometheus_metrics",
				Timerange:   models.TimeRange{Start: now.Add(-24 * time.Hour), End: now},
				Metrics:     []string{"cpu_usage", "memory_usage", "request_latency"},
				SampleCount: 86400,
				Features:    []string{"value", "rate_of_change", "seasonality"},
			},
			Parameters: map[string]interface{}{
				"threshold_std_dev": 3.0,
				"window_size":       60,
				"sensitivity":       0.8,
			},
			LastTrained: now.Add(-time.Hour),
			CreatedAt:   now.Add(-2 * time.Hour),
			UpdatedAt:   now.Add(-time.Hour),
		},
		{
			ID:       uuid.New().String(),
			Name:     "Isolation Forest Detector",
			Type:     "isolation_forest",
			Status:   "active",
			Accuracy: 0.89,
			TrainingData: models.TrainingDataset{
				Source:      "application_metrics",
				Timerange:   models.TimeRange{Start: now.Add(-7 * 24 * time.Hour), End: now},
				Metrics:     []string{"error_rate", "throughput", "response_time"},
				SampleCount: 604800,
				Features:    []string{"value", "trend", "correlation_matrix"},
			},
			Parameters: map[string]interface{}{
				"contamination": 0.1,
				"n_estimators":  100,
				"max_samples":   "auto",
			},
			LastTrained: now.Add(-2 * time.Hour),
			CreatedAt:   now.Add(-4 * time.Hour),
			UpdatedAt:   now.Add(-2 * time.Hour),
		},
		{
			ID:       uuid.New().String(),
			Name:     "LSTM Sequence Detector",
			Type:     "lstm",
			Status:   "training",
			Accuracy: 0.95,
			TrainingData: models.TrainingDataset{
				Source:      "time_series_data",
				Timerange:   models.TimeRange{Start: now.Add(-30 * 24 * time.Hour), End: now},
				Metrics:     []string{"business_metrics", "system_load", "user_activity"},
				SampleCount: 2592000,
				Features:    []string{"sequence_patterns", "temporal_dependencies", "multi_variate"},
			},
			Parameters: map[string]interface{}{
				"sequence_length": 120,
				"hidden_units":    64,
				"dropout_rate":    0.2,
				"learning_rate":   0.001,
			},
			LastTrained: now.Add(-30 * time.Minute),
			CreatedAt:   now.Add(-6 * time.Hour),
			UpdatedAt:   now.Add(-30 * time.Minute),
		},
	}

	for _, model := range models {
		s.anomalyModels[model.ID] = model
	}

	s.logger.Info("Initialized ML models for anomaly detection", zap.Int("model_count", len(models)))
}

// initializeBaselines sets up performance baselines
func (s *IntelligenceService) initializeBaselines() {
	baselines := map[string]models.InsightMetrics{
		"api_service": {
			BaselineLatency:    50.0,
			CurrentLatency:     52.3,
			BaselineThroughput: 1000.0,
			CurrentThroughput:  985.2,
			TrendDirection:     "degrading",
		},
		"database_service": {
			BaselineLatency:    15.0,
			CurrentLatency:     14.8,
			BaselineThroughput: 2500.0,
			CurrentThroughput:  2510.1,
			TrendDirection:     "improving",
		},
		"cache_service": {
			BaselineLatency:    2.0,
			CurrentLatency:     2.1,
			BaselineThroughput: 5000.0,
			CurrentThroughput:  4950.0,
			TrendDirection:     "stable",
		},
	}

	s.performanceBaseline = baselines
	s.logger.Info("Initialized performance baselines", zap.Int("service_count", len(baselines)))
}

// DetectAnomalies performs anomaly detection using ML models
func (s *IntelligenceService) DetectAnomalies(ctx context.Context, metricName string, values []float64, timestamps []time.Time) ([]*models.AnomalyScore, error) {
	s.logger.Info("Running anomaly detection", zap.String("metric", metricName), zap.Int("data_points", len(values)))

	var allScores []*models.AnomalyScore

	// Run detection with each active model
	for modelID, model := range s.anomalyModels {
		if model.Status != "active" {
			continue
		}

		scores, err := s.runAnomalyDetection(model, metricName, values, timestamps)
		if err != nil {
			s.logger.Error("Failed to run anomaly detection", zap.String("model_id", modelID), zap.Error(err))
			continue
		}

		allScores = append(allScores, scores...)
	}

	// Update metrics
	anomalyCount := 0
	for _, score := range allScores {
		if score.IsAnomaly {
			anomalyCount++
		}
	}

	s.intelligenceMetrics.AnomaliesDetected += int64(anomalyCount)
	s.intelligenceMetrics.TimeToDetection = 45.5 // Simulated average detection time

	return allScores, nil
}

// runAnomalyDetection executes anomaly detection for a specific model
func (s *IntelligenceService) runAnomalyDetection(model *models.AnomalyDetectionModel, metricName string, values []float64, timestamps []time.Time) ([]*models.AnomalyScore, error) {
	var scores []*models.AnomalyScore

	switch model.Type {
	case "statistical":
		scores = s.statisticalAnomalyDetection(model, metricName, values, timestamps)
	case "isolation_forest":
		scores = s.isolationForestDetection(model, metricName, values, timestamps)
	case "lstm":
		scores = s.lstmAnomalyDetection(model, metricName, values, timestamps)
	default:
		return nil, fmt.Errorf("unsupported model type: %s", model.Type)
	}

	return scores, nil
}

// statisticalAnomalyDetection implements statistical-based anomaly detection
func (s *IntelligenceService) statisticalAnomalyDetection(model *models.AnomalyDetectionModel, metricName string, values []float64, timestamps []time.Time) []*models.AnomalyScore {
	if len(values) == 0 {
		return nil
	}

	// Calculate mean and standard deviation
	mean := s.calculateMean(values)
	stdDev := s.calculateStdDev(values, mean)
	threshold := stdDev * model.Parameters["threshold_std_dev"].(float64)

	var scores []*models.AnomalyScore

	for i, value := range values {
		deviation := math.Abs(value - mean)
		score := deviation / (threshold + 1e-9) // Avoid division by zero
		isAnomaly := score > 1.0
		confidence := math.Min(score, 1.0)

		scores = append(scores, &models.AnomalyScore{
			Timestamp:  timestamps[i],
			MetricName: metricName,
			Value:      value,
			Score:      score,
			Threshold:  threshold,
			IsAnomaly:  isAnomaly,
			Confidence: confidence,
			Context: map[string]interface{}{
				"mean":    mean,
				"std_dev": stdDev,
				"method":  "statistical",
			},
			ModelID: model.ID,
		})
	}

	return scores
}

// isolationForestDetection implements isolation forest anomaly detection
func (s *IntelligenceService) isolationForestDetection(model *models.AnomalyDetectionModel, metricName string, values []float64, timestamps []time.Time) []*models.AnomalyScore {
	var scores []*models.AnomalyScore

	// Simplified isolation forest simulation
	contamination := model.Parameters["contamination"].(float64)
	threshold := 0.5 + contamination*0.3

	for i, value := range values {
		// Simulate isolation score (in real implementation, this would use actual IF algorithm)
		normalizedValue := (value - s.calculateMean(values)) / (s.calculateStdDev(values, s.calculateMean(values)) + 1e-9)
		score := 1.0 / (1.0 + math.Exp(-math.Abs(normalizedValue))) // Sigmoid-like scoring
		isAnomaly := score > threshold

		scores = append(scores, &models.AnomalyScore{
			Timestamp:  timestamps[i],
			MetricName: metricName,
			Value:      value,
			Score:      score,
			Threshold:  threshold,
			IsAnomaly:  isAnomaly,
			Confidence: score,
			Context: map[string]interface{}{
				"contamination": contamination,
				"method":        "isolation_forest",
			},
			ModelID: model.ID,
		})
	}

	return scores
}

// lstmAnomalyDetection implements LSTM-based anomaly detection
func (s *IntelligenceService) lstmAnomalyDetection(model *models.AnomalyDetectionModel, metricName string, values []float64, timestamps []time.Time) []*models.AnomalyScore {
	var scores []*models.AnomalyScore

	sequenceLength := int(model.Parameters["sequence_length"].(float64))
	if len(values) < sequenceLength {
		return scores
	}

	for i := sequenceLength; i < len(values); i++ {
		// Simulate LSTM prediction error (in real implementation, this would use actual LSTM model)
		sequence := values[i-sequenceLength : i]
		predicted := s.calculateMean(sequence) // Simplified prediction
		actual := values[i]
		predictionError := math.Abs(actual - predicted)

		// Convert error to anomaly score
		score := predictionError / (actual + 1e-9)
		threshold := 0.15 // 15% prediction error threshold
		isAnomaly := score > threshold

		scores = append(scores, &models.AnomalyScore{
			Timestamp:  timestamps[i],
			MetricName: metricName,
			Value:      actual,
			Score:      score,
			Threshold:  threshold,
			IsAnomaly:  isAnomaly,
			Confidence: math.Min(score*2, 1.0),
			Context: map[string]interface{}{
				"predicted":        predicted,
				"prediction_error": predictionError,
				"method":           "lstm",
			},
			ModelID: model.ID,
		})
	}

	return scores
}

// GeneratePredictiveAlerts creates predictive alerts based on trends
func (s *IntelligenceService) GeneratePredictiveAlerts(ctx context.Context, metricData map[string][]float64) ([]*models.PredictiveAlert, error) {
	s.logger.Info("Generating predictive alerts", zap.Int("metrics", len(metricData)))

	var alerts []*models.PredictiveAlert

	for metricName, values := range metricData {
		if len(values) < 10 { // Need minimum data points for prediction
			continue
		}

		// Analyze trend and predict future values
		trend := s.calculateTrend(values)
		prediction := s.predictFutureValue(values, 30*time.Minute) // Predict 30 minutes ahead

		// Generate alert if prediction exceeds thresholds
		if alert := s.createPredictiveAlert(metricName, values[len(values)-1], prediction, trend); alert != nil {
			alerts = append(alerts, alert)
		}
	}

	s.predictiveAlerts = append(s.predictiveAlerts, alerts...)
	s.intelligenceMetrics.PredictionsGenerated += int64(len(alerts))

	return alerts, nil
}

// createPredictiveAlert creates a predictive alert if conditions are met
func (s *IntelligenceService) createPredictiveAlert(metricName string, currentValue, predictedValue, trend float64) *models.PredictiveAlert {
	// Define thresholds for different metrics
	thresholds := map[string]float64{
		"cpu_usage":     80.0,
		"memory_usage":  85.0,
		"disk_usage":    90.0,
		"error_rate":    5.0,
		"response_time": 1000.0,
	}

	threshold, exists := thresholds[metricName]
	if !exists {
		threshold = 100.0 // Default threshold
	}

	// Check if prediction exceeds threshold
	if predictedValue <= threshold {
		return nil
	}

	probability := math.Min((predictedValue-threshold)/threshold, 1.0)
	if probability < 0.3 { // Only alert if probability > 30%
		return nil
	}

	severity := "warning"
	if probability > 0.7 {
		severity = "critical"
	}

	timeToEvent := time.Duration(float64(time.Hour) / (math.Abs(trend) + 1e-9))

	return &models.PredictiveAlert{
		ID:     uuid.New().String(),
		RuleID: fmt.Sprintf("predictive_%s", metricName),
		Prediction: models.Prediction{
			Type:           "threshold_breach",
			Description:    fmt.Sprintf("%s is predicted to exceed threshold", metricName),
			Metric:         metricName,
			CurrentValue:   currentValue,
			PredictedValue: predictedValue,
			Threshold:      threshold,
			Confidence:     probability,
			Factors:        []string{"trending_upward", "historical_pattern", "seasonal_analysis"},
		},
		Probability:     probability,
		TimeToEvent:     timeToEvent,
		Severity:        severity,
		Status:          "active",
		Recommendations: s.generateRecommendationsForAlert(metricName, predictedValue, threshold),
		CreatedAt:       time.Now(),
		UpdatedAt:       time.Now(),
	}
}

// PerformRootCauseAnalysis conducts automated incident investigation
func (s *IntelligenceService) PerformRootCauseAnalysis(ctx context.Context, incidentID string) (*models.RootCauseAnalysis, error) {
	s.logger.Info("Starting root cause analysis", zap.String("incident_id", incidentID))

	analysisID := uuid.New().String()
	now := time.Now()

	// Simulate comprehensive root cause analysis
	analysis := &models.RootCauseAnalysis{
		ID:         analysisID,
		IncidentID: incidentID,
		Status:     "in_progress",
		Confidence: 0.0,
		CreatedAt:  now,
	}

	// Build timeline of events
	analysis.Timeline = s.buildIncidentTimeline(incidentID, now)

	// Identify correlations
	analysis.Correlations = s.identifyCorrelations()

	// Find root causes
	analysis.RootCauses = s.identifyRootCauses(incidentID, analysis.Timeline)

	// Calculate overall confidence
	analysis.Confidence = s.calculateAnalysisConfidence(analysis.RootCauses)
	analysis.Status = "completed"
	completedTime := time.Now()
	analysis.CompletedAt = &completedTime

	s.activeAnalyses[analysisID] = analysis

	return analysis, nil
}

// buildIncidentTimeline creates a timeline of events for the incident
func (s *IntelligenceService) buildIncidentTimeline(incidentID string, incidentTime time.Time) []models.TimelineEvent {
	events := []models.TimelineEvent{
		{
			Timestamp:   incidentTime.Add(-15 * time.Minute),
			Type:        "metric_anomaly",
			Component:   "api_gateway",
			Description: "Response time increased to 850ms (baseline: 50ms)",
			Severity:    "warning",
			Data: map[string]interface{}{
				"metric":   "response_time",
				"value":    850.0,
				"baseline": 50.0,
			},
		},
		{
			Timestamp:   incidentTime.Add(-10 * time.Minute),
			Type:        "log_error",
			Component:   "database",
			Description: "Connection pool exhaustion detected",
			Severity:    "error",
			Data: map[string]interface{}{
				"error_type":         "connection_pool_exhausted",
				"active_connections": 100,
				"max_connections":    100,
			},
		},
		{
			Timestamp:   incidentTime.Add(-5 * time.Minute),
			Type:        "alert_triggered",
			Component:   "monitoring_system",
			Description: "High error rate alert triggered",
			Severity:    "critical",
			Data: map[string]interface{}{
				"alert_rule":    "high_error_rate",
				"threshold":     5.0,
				"current_value": 12.5,
			},
		},
		{
			Timestamp:   incidentTime,
			Type:        "incident_created",
			Component:   "incident_management",
			Description: "Incident created due to service degradation",
			Severity:    "critical",
			Data: map[string]interface{}{
				"incident_id": incidentID,
				"trigger":     "automated_alert",
			},
		},
	}

	return events
}

// identifyCorrelations finds correlations between metrics
func (s *IntelligenceService) identifyCorrelations() []models.Correlation {
	return []models.Correlation{
		{
			MetricA:     "response_time",
			MetricB:     "database_connections",
			Coefficient: 0.85,
			Strength:    "strong",
			Type:        "positive",
			Timelag:     2 * time.Minute,
		},
		{
			MetricA:     "error_rate",
			MetricB:     "cpu_usage",
			Coefficient: 0.72,
			Strength:    "moderate",
			Type:        "positive",
			Timelag:     30 * time.Second,
		},
		{
			MetricA:     "throughput",
			MetricB:     "response_time",
			Coefficient: -0.68,
			Strength:    "moderate",
			Type:        "negative",
			Timelag:     1 * time.Minute,
		},
	}
}

// identifyRootCauses identifies potential root causes
func (s *IntelligenceService) identifyRootCauses(incidentID string, timeline []models.TimelineEvent) []models.RootCause {
	return []models.RootCause{
		{
			ID:          uuid.New().String(),
			Type:        "resource",
			Component:   "database",
			Description: "Database connection pool exhaustion due to increased load",
			Evidence: []models.Evidence{
				{
					Type:        "metric",
					Source:      "prometheus",
					Description: "Database connection count reached maximum (100/100)",
					Data: map[string]interface{}{
						"metric_name": "db_connections_active",
						"value":       100,
						"max_value":   100,
					},
					Timestamp: timeline[1].Timestamp,
					Relevance: 0.95,
				},
				{
					Type:        "log",
					Source:      "application_logs",
					Description: "Connection timeout errors in application logs",
					Data: map[string]interface{}{
						"error_count": 45,
						"error_type":  "connection_timeout",
					},
					Timestamp: timeline[1].Timestamp,
					Relevance: 0.90,
				},
			},
			Probability: 0.92,
			Impact:      "high",
		},
		{
			ID:          uuid.New().String(),
			Type:        "configuration",
			Component:   "load_balancer",
			Description: "Insufficient connection pool configuration for peak load",
			Evidence: []models.Evidence{
				{
					Type:        "configuration",
					Source:      "infrastructure",
					Description: "Connection pool size unchanged despite 3x traffic increase",
					Data: map[string]interface{}{
						"pool_size":      100,
						"recommended":    300,
						"traffic_growth": 3.2,
					},
					Timestamp: timeline[0].Timestamp,
					Relevance: 0.85,
				},
			},
			Probability: 0.78,
			Impact:      "medium",
		},
	}
}

// calculateAnalysisConfidence calculates overall confidence in the analysis
func (s *IntelligenceService) calculateAnalysisConfidence(rootCauses []models.RootCause) float64 {
	if len(rootCauses) == 0 {
		return 0.0
	}

	totalProbability := 0.0
	for _, cause := range rootCauses {
		totalProbability += cause.Probability
	}

	return totalProbability / float64(len(rootCauses))
}

// GeneratePerformanceInsights creates performance optimization insights
func (s *IntelligenceService) GeneratePerformanceInsights(ctx context.Context) ([]*models.PerformanceInsight, error) {
	s.logger.Info("Generating performance insights")

	var insights []*models.PerformanceInsight

	// Analyze each service
	for serviceName, baseline := range s.performanceBaseline {
		insight := s.analyzeServicePerformance(serviceName, baseline)
		if insight != nil {
			insights = append(insights, insight)
		}
	}

	return insights, nil
}

// analyzeServicePerformance analyzes performance for a specific service
func (s *IntelligenceService) analyzeServicePerformance(serviceName string, baseline models.InsightMetrics) *models.PerformanceInsight {
	// Calculate performance impact
	latencyIncrease := baseline.CurrentLatency - baseline.BaselineLatency
	throughputDecrease := baseline.BaselineThroughput - baseline.CurrentThroughput

	if latencyIncrease < 5.0 && throughputDecrease < 50.0 {
		return nil // Performance is acceptable
	}

	severity := "low"
	if latencyIncrease > 20.0 || throughputDecrease > 200.0 {
		severity = "high"
	} else if latencyIncrease > 10.0 || throughputDecrease > 100.0 {
		severity = "medium"
	}

	impact := models.PerformanceImpact{
		Latency:     latencyIncrease,
		Throughput:  (throughputDecrease / baseline.BaselineThroughput) * 100,
		ErrorRate:   0.5,  // Simulated
		ResourceUse: 15.0, // Simulated
	}

	suggestions := s.generatePerformanceSuggestions(serviceName, baseline, impact)

	return &models.PerformanceInsight{
		ID:          uuid.New().String(),
		Type:        "bottleneck",
		Component:   serviceName,
		Title:       fmt.Sprintf("%s Performance Degradation", serviceName),
		Description: fmt.Sprintf("Service showing %.1fms latency increase and %.1f RPS throughput decrease", latencyIncrease, throughputDecrease),
		Severity:    severity,
		Impact:      impact,
		Suggestions: suggestions,
		Metrics:     baseline,
		CreatedAt:   time.Now(),
	}
}

// generatePerformanceSuggestions creates performance improvement suggestions
func (s *IntelligenceService) generatePerformanceSuggestions(serviceName string, baseline models.InsightMetrics, impact models.PerformanceImpact) []models.Recommendation {
	recommendations := []models.Recommendation{
		{
			ID:          uuid.New().String(),
			Type:        "optimization",
			Priority:    "high",
			Title:       "Increase Database Connection Pool",
			Description: "Database connection pool appears to be a bottleneck. Increase pool size to handle peak load.",
			Impact:      "Reduce latency by 20-30ms, increase throughput by 15%",
			Effort:      "low",
			Actions: []models.RecommendedAction{
				{
					Type:        "scale_up",
					Description: "Increase database connection pool from 100 to 200",
					Parameters: map[string]interface{}{
						"current_pool_size": 100,
						"recommended_size":  200,
						"service":           serviceName,
					},
					Automated: true,
				},
			},
			Metrics: models.RecommendationMetrics{
				PerformanceGain: 25.0,
				CostSavings:     0.0,
				ResourceSavings: 0.0,
				ROI:             5.2,
			},
			CreatedAt: time.Now(),
		},
		{
			ID:          uuid.New().String(),
			Type:        "scaling",
			Priority:    "medium",
			Title:       "Enable Database Read Replicas",
			Description: "Distribute read queries across multiple database replicas to reduce primary database load.",
			Impact:      "Reduce database load by 40%, improve read query performance",
			Effort:      "medium",
			Actions: []models.RecommendedAction{
				{
					Type:        "configuration",
					Description: "Configure read replica routing for read-only queries",
					Parameters: map[string]interface{}{
						"replica_count": 2,
						"read_ratio":    0.7,
						"service":       serviceName,
					},
					Automated: false,
				},
			},
			Metrics: models.RecommendationMetrics{
				PerformanceGain: 35.0,
				CostSavings:     0.0,
				ResourceSavings: 0.0,
				ROI:             3.8,
			},
			CreatedAt: time.Now(),
		},
	}

	return recommendations
}

// CreateCapacityPlan generates capacity planning recommendations
func (s *IntelligenceService) CreateCapacityPlan(ctx context.Context, serviceName string, timeHorizon time.Duration) (*models.CapacityPlan, error) {
	s.logger.Info("Creating capacity plan", zap.String("service", serviceName), zap.Duration("horizon", timeHorizon))

	planID := uuid.New().String()
	now := time.Now()

	// Generate resource forecasts
	forecast := s.generateResourceForecast(serviceName, timeHorizon)

	// Generate capacity recommendations
	recommendations := s.generateCapacityRecommendations(serviceName, forecast)

	// Generate cost analysis
	costAnalysis := s.generateCostAnalysis(serviceName, forecast, recommendations)

	plan := &models.CapacityPlan{
		ID:              planID,
		Service:         serviceName,
		TimeHorizon:     timeHorizon,
		Forecast:        forecast,
		Recommendations: recommendations,
		CostAnalysis:    costAnalysis,
		CreatedAt:       now,
	}

	s.capacityPlans[planID] = plan

	return plan, nil
}

// generateResourceForecast creates resource usage forecasts
func (s *IntelligenceService) generateResourceForecast(serviceName string, timeHorizon time.Duration) models.ResourceForecast {
	// Simulate resource forecasting based on historical trends
	now := time.Now()

	// Generate sample timeline data
	var cpuTimeline, memoryTimeline, storageTimeline, networkTimeline []models.DataPoint

	hours := int(timeHorizon.Hours())
	for i := 0; i <= hours; i += 6 { // Data points every 6 hours
		timestamp := now.Add(time.Duration(i) * time.Hour)

		// Simulate growth trends
		growthFactor := 1.0 + (float64(i)/float64(hours))*0.3 // 30% growth over time horizon

		cpuTimeline = append(cpuTimeline, models.DataPoint{
			Timestamp: timestamp,
			Value:     45.0*growthFactor + rand.Float64()*10.0, // Base 45% + growth + noise
		})

		memoryTimeline = append(memoryTimeline, models.DataPoint{
			Timestamp: timestamp,
			Value:     60.0*growthFactor + rand.Float64()*15.0, // Base 60% + growth + noise
		})

		storageTimeline = append(storageTimeline, models.DataPoint{
			Timestamp: timestamp,
			Value:     70.0*growthFactor + rand.Float64()*8.0, // Base 70% + growth + noise
		})

		networkTimeline = append(networkTimeline, models.DataPoint{
			Timestamp: timestamp,
			Value:     30.0*growthFactor + rand.Float64()*12.0, // Base 30% + growth + noise
		})
	}

	return models.ResourceForecast{
		CPU: models.ResourceProjection{
			Current:    45.0,
			Projected:  58.5,
			Peak:       75.2,
			Average:    52.3,
			Trend:      "increasing",
			Confidence: 0.87,
			Timeline:   cpuTimeline,
		},
		Memory: models.ResourceProjection{
			Current:    60.0,
			Projected:  78.0,
			Peak:       89.5,
			Average:    69.2,
			Trend:      "increasing",
			Confidence: 0.91,
			Timeline:   memoryTimeline,
		},
		Storage: models.ResourceProjection{
			Current:    70.0,
			Projected:  91.0,
			Peak:       95.8,
			Average:    82.1,
			Trend:      "increasing",
			Confidence: 0.93,
			Timeline:   storageTimeline,
		},
		Network: models.ResourceProjection{
			Current:    30.0,
			Projected:  39.0,
			Peak:       52.1,
			Average:    34.5,
			Trend:      "increasing",
			Confidence: 0.84,
			Timeline:   networkTimeline,
		},
	}
}

// generateCapacityRecommendations creates capacity recommendations
func (s *IntelligenceService) generateCapacityRecommendations(serviceName string, forecast models.ResourceForecast) []models.CapacityRecommendation {
	var recommendations []models.CapacityRecommendation
	now := time.Now()

	// CPU scaling recommendation
	if forecast.CPU.Projected > 80.0 {
		recommendations = append(recommendations, models.CapacityRecommendation{
			Type:      "scale_up",
			Component: "cpu",
			Action:    "Increase CPU allocation by 50%",
			Timing:    now.Add(7 * 24 * time.Hour), // 1 week lead time
			Parameters: map[string]interface{}{
				"current_cores":  4,
				"target_cores":   6,
				"scaling_factor": 1.5,
			},
			CostImpact: 45.0, // $45/month additional
			Urgency:    "high",
		})
	}

	// Memory scaling recommendation
	if forecast.Memory.Projected > 85.0 {
		recommendations = append(recommendations, models.CapacityRecommendation{
			Type:      "scale_up",
			Component: "memory",
			Action:    "Increase memory allocation by 40%",
			Timing:    now.Add(5 * 24 * time.Hour), // 5 days lead time
			Parameters: map[string]interface{}{
				"current_memory_gb": 8,
				"target_memory_gb":  12,
				"scaling_factor":    1.4,
			},
			CostImpact: 32.0, // $32/month additional
			Urgency:    "high",
		})
	}

	// Storage scaling recommendation
	if forecast.Storage.Projected > 90.0 {
		recommendations = append(recommendations, models.CapacityRecommendation{
			Type:      "scale_up",
			Component: "storage",
			Action:    "Add additional storage volume",
			Timing:    now.Add(14 * 24 * time.Hour), // 2 weeks lead time
			Parameters: map[string]interface{}{
				"current_storage_gb":    100,
				"additional_storage_gb": 50,
				"storage_type":          "ssd",
			},
			CostImpact: 25.0, // $25/month additional
			Urgency:    "medium",
		})
	}

	return recommendations
}

// generateCostAnalysis creates cost optimization analysis
func (s *IntelligenceService) generateCostAnalysis(serviceName string, forecast models.ResourceForecast, recommendations []models.CapacityRecommendation) models.CostAnalysis {
	currentCost := 150.0 // Current monthly cost
	projectedCost := currentCost

	// Calculate cost impact of recommendations
	for _, rec := range recommendations {
		projectedCost += rec.CostImpact
	}

	// Calculate cost breakdown
	breakdown := map[string]float64{
		"compute": 80.0,
		"storage": 30.0,
		"network": 25.0,
		"other":   15.0,
	}

	// Generate cost optimizations
	optimizations := []models.CostOptimization{
		{
			Type:        "rightsizing",
			Description: "Rightsize over-provisioned instances during off-peak hours",
			Savings:     22.0,
			Effort:      "low",
			Risk:        "low",
			Priority:    1,
		},
		{
			Type:        "unused_resources",
			Description: "Remove unused development environments",
			Savings:     15.0,
			Effort:      "low",
			Risk:        "low",
			Priority:    2,
		},
		{
			Type:        "reserved_instances",
			Description: "Purchase reserved instances for stable workloads",
			Savings:     35.0,
			Effort:      "medium",
			Risk:        "low",
			Priority:    3,
		},
	}

	totalSavings := 0.0
	for _, opt := range optimizations {
		totalSavings += opt.Savings
	}

	return models.CostAnalysis{
		CurrentCost:   currentCost,
		ProjectedCost: projectedCost,
		Savings:       totalSavings,
		Breakdown:     breakdown,
		Optimizations: optimizations,
	}
}

// generateRecommendationsForAlert creates recommendations for predictive alerts
func (s *IntelligenceService) generateRecommendationsForAlert(metricName string, predictedValue, threshold float64) []models.Recommendation {
	recommendations := []models.Recommendation{
		{
			ID:          uuid.New().String(),
			Type:        "scaling",
			Priority:    "high",
			Title:       fmt.Sprintf("Scale %s resources before threshold breach", metricName),
			Description: fmt.Sprintf("Proactively scale resources to prevent %s from reaching %.1f", metricName, threshold),
			Impact:      "Prevent service degradation",
			Effort:      "low",
			Actions: []models.RecommendedAction{
				{
					Type:        "scale_up",
					Description: "Increase resource allocation by 20%",
					Automated:   true,
				},
			},
			CreatedAt: time.Now(),
		},
	}

	// Convert to pointers for storage
	for i := range recommendations {
		s.recommendations = append(s.recommendations, &recommendations[i])
	}
	s.intelligenceMetrics.RecommendationsCreated += int64(len(recommendations))

	return recommendations
}

// Helper functions
func (s *IntelligenceService) calculateMean(values []float64) float64 {
	if len(values) == 0 {
		return 0.0
	}
	sum := 0.0
	for _, v := range values {
		sum += v
	}
	return sum / float64(len(values))
}

func (s *IntelligenceService) calculateStdDev(values []float64, mean float64) float64 {
	if len(values) <= 1 {
		return 0.0
	}
	sum := 0.0
	for _, v := range values {
		sum += math.Pow(v-mean, 2)
	}
	return math.Sqrt(sum / float64(len(values)-1))
}

func (s *IntelligenceService) calculateTrend(values []float64) float64 {
	if len(values) < 2 {
		return 0.0
	}

	// Simple linear trend calculation
	n := float64(len(values))
	sumX, sumY, sumXY, sumX2 := 0.0, 0.0, 0.0, 0.0

	for i, y := range values {
		x := float64(i)
		sumX += x
		sumY += y
		sumXY += x * y
		sumX2 += x * x
	}

	slope := (n*sumXY - sumX*sumY) / (n*sumX2 - sumX*sumX)
	return slope
}

func (s *IntelligenceService) predictFutureValue(values []float64, duration time.Duration) float64 {
	if len(values) < 2 {
		return values[0]
	}

	// Simple linear extrapolation
	trend := s.calculateTrend(values)
	lastValue := values[len(values)-1]

	// Predict based on duration (simplified)
	timeSteps := duration.Minutes() / 5.0 // Assuming 5-minute intervals

	return lastValue + trend*timeSteps
}

// GetIntelligenceMetrics returns current intelligence metrics
func (s *IntelligenceService) GetIntelligenceMetrics() models.IntelligenceMetrics {
	// Update accuracy metrics
	s.intelligenceMetrics.AccuracyRate = 0.91
	s.intelligenceMetrics.FalsePositiveRate = 0.08
	s.intelligenceMetrics.CostSavingsRealized = 1250.0
	s.intelligenceMetrics.PerformanceImprovements = 23.5

	return s.intelligenceMetrics
}

// GetActiveModels returns all active ML models
func (s *IntelligenceService) GetActiveModels() []*models.AnomalyDetectionModel {
	var models []*models.AnomalyDetectionModel
	for _, model := range s.anomalyModels {
		models = append(models, model)
	}
	return models
}

// GetPredictiveAlerts returns active predictive alerts
func (s *IntelligenceService) GetPredictiveAlerts() []*models.PredictiveAlert {
	// Filter active alerts
	var activeAlerts []*models.PredictiveAlert
	for _, alert := range s.predictiveAlerts {
		if alert.Status == "active" {
			activeAlerts = append(activeAlerts, alert)
		}
	}
	return activeAlerts
}

// GetRecommendations returns all recommendations
func (s *IntelligenceService) GetRecommendations() []*models.Recommendation {
	// Sort by priority and creation time
	recommendations := make([]*models.Recommendation, len(s.recommendations))
	copy(recommendations, s.recommendations)

	sort.Slice(recommendations, func(i, j int) bool {
		priorityOrder := map[string]int{"high": 3, "medium": 2, "low": 1}
		if priorityOrder[recommendations[i].Priority] != priorityOrder[recommendations[j].Priority] {
			return priorityOrder[recommendations[i].Priority] > priorityOrder[recommendations[j].Priority]
		}
		return recommendations[i].CreatedAt.After(recommendations[j].CreatedAt)
	})

	return recommendations
}

// GetCapacityPlans returns all capacity plans
func (s *IntelligenceService) GetCapacityPlans() []*models.CapacityPlan {
	var plans []*models.CapacityPlan
	for _, plan := range s.capacityPlans {
		plans = append(plans, plan)
	}
	return plans
}
